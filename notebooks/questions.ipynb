{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "● Comment gérer des transformations de données pour de la grande volumétrie ?\n",
    "\n",
    "- Je recommande Apache Spark afin de traiter les données en parallèle via le partitionnement.\n",
    "- Les fichiers doivent être enregistrés au format colonnes (parquet) puis compresssé (snappy).\n",
    "- Côté infrastructure, j'utilise des outils Cloud qui permettent de rendre les performances scalables (Cloud Provider type AWS/GCP mais il y a également d'autres outils managés comme Databricks et Snowflake)\n",
    "\n",
    "● Quelles sont les différentes étapes d’un projet data ?\n",
    "\n",
    "1. Compréhension du besoin métier et identification des objectifs\n",
    "2. Identification, collecte et ingestion des données\n",
    "3. Exploration des données\n",
    "4. Préparation des données (mise en place de méthode CI/CD, IaC et versionning)\n",
    "5. Modélisation des tables dans un Data Warehouse\n",
    "6. Automatisation, tests et monitoring\n",
    "7. Documentation\n",
    "8. Mise en production\n",
    "9. Maintenance, support et amélioration continue\n",
    "\n",
    "● Quels outils utilisez-vous en plus pour que cet exercice devienne un vrai cas d'usage en entreprise ?\n",
    "\n",
    "- Ingestion, exploration : Pandas, PySpark, SQL\n",
    "- Transformation des données : DBT, PySpark/Spark SQL, outils ELT/ETL\n",
    "- Versioning et CI/CD : Git (Github Actions, Gitlab), Terraform pour l'IaC\n",
    "- Stockage et traitement distribué : Snowflake, Databricks, Redshift, BigQuery\n",
    "- Monitoring : Datadog\n",
    "- Documentation : Intégrée dans le code (via DBT par exemple), Confluence. On peut également utiliser des sites spécifiques comme dans le cadre SNDS/OMOP avec les documentations Athena, OHDSI et celles du Health Data Hub\n",
    "\n",
    "● Quelle méthodologie de travail serait adaptée à un projet data ?\n",
    "\n",
    "- Méthodologie agile avec des sprints adaptés à la charge de travail\n",
    "- Pratiques DataOps pour industrialiser les pipelines : automatisation des flux et des tests, versioning, monitoring, code réutilisable\n",
    "- Documentation et gouvernance des données pour assurer la qualité, la traçabilité et la conformité des données (RGPD).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
